# ==========================================
# üìå LINEAR REGRESSION ‚Äì CALIFORNIA HOUSING
# ==========================================

# ---- IMPORTS ----
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ---- Load Dataset ----
data = fetch_california_housing()
X = data.data
y = data.target

# ---- Train-Test Split ----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ---- Train Model ----
model = LinearRegression()
model.fit(X_train, y_train)

# ---- Predict ----
y_pred = model.predict(X_test)

# ---- Evaluation ----
print("===== Linear Regression on California Housing =====")
print("MSE:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))

# ---- Visualization ----
plt.figure(figsize=(8,6))

# Scatter Plot
plt.scatter(y_test, y_pred, alpha=0.4, label="Predicted Points")

# Best-fit Line
plt.plot(y_test, m * y_test + c, color='red', linewidth=2, label="Best Fit Line")

# Labels and Style
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted ‚Äî Linear Regression")
plt.grid(True)
plt.legend()

plt.show()

# ==========================================
# üìå LOGISTIC REGRESSION ‚Äì IRIS (BINARY)
# ==========================================

# ---- IMPORTS ----
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix
)

# ---- Load Dataset ----
iris = load_iris()
X = iris.data
y = iris.target

# ---- Train-Test Split ----
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ---- Train Model ----
model = LogisticRegression("iter"= 500)
model.fit(X_train, y_train)

# ---- Predict ----
y_pred = model.predict(X_test)

# ---- Evaluation ----
print("===== Logistic Regression on IRIS (Binary) =====")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ---- Confusion Matrix ----
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, cmap='Blues', fmt="d")
plt.title("Confusion Matrix ‚Äî Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ---- 2D Scatter Visualization (Feature 1 vs Feature 2) ----
plt.figure(figsize=(8,6))
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap="viridis", alpha=0.8)
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.title("Logistic Regression (Binary) ‚Äî Classification Plot")
plt.show()


# ==========================================
# üìå K-MEANS CLUSTERING ‚Äî WINE DATASET (with Silhouette Score)
# ==========================================

# ---- IMPORTS ----
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ---- Load Dataset ----
wine = load_wine()
X = pd.DataFrame(wine.data, columns=wine.feature_names)
y = wine.target        # actual wine classes

# ---- Standardize ----
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---- Apply K-Means ----
k = 3
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# ---- Silhouette Score ----
score = silhouette_score(X_scaled, clusters)
print(f"Silhouette Score (k={k}): {score:.4f}")

# ---- Crosstab: Cluster vs Actual Class ----
print("\nCluster vs Actual Class:")
print(pd.crosstab(y, clusters))

# ---- Visualization (Alcohol vs Flavanoids) ----
plt.figure(figsize=(8,6))
plt.scatter(X["alcohol"], X["flavanoids"], c=clusters, cmap="viridis", s=60)
plt.xlabel("Alcohol")
plt.ylabel("Flavanoids")
plt.title(f"K-Means Clustering (k={k}) ‚Äî Wine Dataset")
plt.show()


# ==========================================
# üìå PCA (Principal Component Analysis) ‚Äî Wine Dataset
# ==========================================

# ---- IMPORTS ----
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ---- Load Dataset ----
wine = load_wine()
X = pd.DataFrame(wine.data, columns=wine.feature_names)
y = wine.target  # actual wine classes

# ---- Standardize Features ----
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ---- Apply PCA (reduce to 2 components) ----
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# ---- Convert PCA result to DataFrame ----
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
df_pca['Actual Class'] = y

# ---- Explained Variance Ratio ----
print("Explained Variance Ratio (PC1, PC2):", pca.explained_variance_ratio_)

# ---- Scatter Plot (2D PCA) colored by Actual Class ----
plt.figure(figsize=(8,6))
plt.scatter(df_pca['PC1'], df_pca['PC2'], c=df_pca['Actual Class'], cmap='viridis', s=60)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA (2 Components) ‚Äî Wine Dataset")
plt.colorbar(label='Actual Class')
plt.show()


# ==============================================
# PCA on MNIST Dataset using Keras - Organized Script
# ==============================================

#1Ô∏è‚É£ Import Libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from tensorflow.keras.datasets import mnist

# 2Ô∏è‚É£ Load MNIST Dataset
print("Loading MNIST dataset using Keras...")
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X = np.concatenate((X_train, X_test), axis=0)  # Shape: (70000, 28, 28)
y = np.concatenate((y_train, y_test), axis=0)  # Labels 0-9
print(f"Original data shape: {X.shape}, Labels shape: {y.shape}")

# Flatten images for PCA
X_flat = X.reshape(X.shape[0], -1)  # Shape: (70000, 784)
print(f"Flattened data shape: {X_flat.shape}")

# 3Ô∏è‚É£ PCA for Visualization (2D)
print("Applying PCA to reduce to 2D for visualization...")
pca_2d = PCA(n_components=2)
X_pca_2d = pca_2d.fit_transform(X_flat)
print(f"Reduced shape (2D): {X_pca_2d.shape}")

# 4Ô∏è‚É£ Plot PCA 2D Scatter
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='tab10', alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('MNIST PCA (2D Visualization)')
plt.colorbar(scatter, ticks=range(10))
plt.show()

# 5Ô∏è‚É£ PCA for Reconstruction
n_components = 50  # Number of components for reconstruction
print(f"Applying PCA with {n_components} components for reconstruction...")
pca_reconstruct = PCA(n_components=n_components)
X_reduced = pca_reconstruct.fit_transform(X_flat)
X_reconstructed = pca_reconstruct.inverse_transform(X_reduced)

# 6Ô∏è‚É£ Show Original vs Reconstructed Image
plt.figure(figsize=(8, 4))

# Original Image
plt.subplot(1, 2, 1)
plt.imshow(X_flat[0].reshape(28, 28), cmap='gray')
plt.title('Original Image')

# Reconstructed Image
plt.subplot(1, 2, 2)
plt.imshow(X_reconstructed[0].reshape(28, 28), cmap='gray')
plt.title(f'Reconstructed with {n_components} PCs')

plt.show()


import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# -------------------------------
# 1. LOAD & PREPROCESS DATA
# -------------------------------
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)

# One-hot encode labels
encoder = OneHotEncoder(sparse_output=False)
y = encoder.fit_transform(y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# -------------------------------
# 2. ACTIVATION FUNCTIONS
# -------------------------------
def relu(z):
    return np.maximum(0, z)

def relu_derivative(a):
    return (a > 0).astype(float)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

# -------------------------------
# 3. INITIALIZE PARAMETERS
# -------------------------------
input_size = X_train.shape[1]   # 4 features
hidden_size = 10                # Increased hidden neurons
output_size = y_train.shape[1]  # 3 classes
lr = 0.01
epochs = 500

np.random.seed(42)
# He initialization for ReLU
W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / input_size)
b1 = np.zeros((1, hidden_size))
W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / hidden_size)
b2 = np.zeros((1, output_size))

# -------------------------------
# 4. TRAINING LOOP
# -------------------------------
train_loss_history = []
test_loss_history = []
train_acc_history = []
test_acc_history = []

for epoch in range(epochs):
    # ---- Forward Propagation ----
    Z1 = np.dot(X_train, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)

    # ---- Compute Loss ----
    m = y_train.shape[0]
    train_loss = -np.sum(y_train * np.log(A2 + 1e-8)) / m
    train_loss_history.append(train_loss)

    # ---- Compute Accuracy ----
    y_pred_train = np.argmax(A2, axis=1)
    y_true_train = np.argmax(y_train, axis=1)
    train_acc = np.mean(y_pred_train == y_true_train)
    train_acc_history.append(train_acc)

    # ---- Backpropagation ----
    dZ2 = A2 - y_train
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(A1)
    dW1 = np.dot(X_train.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    # ---- Update parameters ----
    W1 -= lr * dW1
    b1 -= lr * db1
    W2 -= lr * dW2
    b2 -= lr * db2

    # ---- Evaluate Test Set ----
    Z1_test = np.dot(X_test, W1) + b1
    A1_test = relu(Z1_test)
    Z2_test = np.dot(A1_test, W2) + b2
    A2_test = softmax(Z2_test)

    m_test = y_test.shape[0]
    test_loss = -np.sum(y_test * np.log(A2_test + 1e-8)) / m_test
    test_loss_history.append(test_loss)

    y_pred_test = np.argmax(A2_test, axis=1)
    y_true_test = np.argmax(y_test, axis=1)
    test_acc = np.mean(y_pred_test == y_true_test)
    test_acc_history.append(test_acc)

    if epoch % 50 == 0:
        print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Test Loss={test_loss:.4f}, "
              f"Train Acc={train_acc:.2f}, Test Acc={test_acc:.2f}")

# -------------------------------
# 5. PLOT LOSS & ACCURACY
# -------------------------------
plt.figure(figsize=(12,5))

# Loss
plt.subplot(1,2,1)
plt.plot(train_loss_history, label='Train Loss', color='red')
plt.plot(test_loss_history, label='Test Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()

# Accuracy
plt.subplot(1,2,2)
plt.plot(train_acc_history, label='Train Accuracy', color='green')
plt.plot(test_acc_history, label='Test Accuracy', color='blue')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Accuracy Curve")
plt.legend()

plt.show()


# ================================================
#   DEEP ARTIFICIAL NEURAL NETWORK (FROM SCRATCH)
#        FOR MNIST HANDWRITTEN DIGIT CLASSIFICATION
# ================================================

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import confusion_matrix, classification_report
np.random.seed(42)

# ------------------------------------------------
# 1. LOAD & PREPROCESS MNIST (NO SSL ISSUES)
# ------------------------------------------------
(X_train, y_train), (X_val, y_val) = mnist.load_data()

# Normalize and flatten (28x28 ‚Üí 784)
X_train = X_train.reshape(-1, 784).astype('float32') / 255.0
X_val   = X_val.reshape(-1, 784).astype('float32') / 255.0

# One-hot encode labels
encoder = OneHotEncoder(sparse_output=False)
y_train = encoder.fit_transform(y_train.reshape(-1,1))
y_val   = encoder.transform(y_val.reshape(-1,1))

# ------------------------------------------------
# 2. ACTIVATION FUNCTIONS
# ------------------------------------------------
def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(float)

def softmax(z):
    e = np.exp(z - np.max(z, axis=1, keepdims=True))
    return e / np.sum(e, axis=1, keepdims=True)

# ------------------------------------------------
# 3. NETWORK ARCHITECTURE + INITIALIZATION (HE INIT)
# ------------------------------------------------
input_size = 784
hidden1 = 128
hidden2 = 64
output_size = 10
dropout_rate = 0.2
l2_lambda = 0.001
lr = 0.001
epochs = 40

def he_init(n_in, n_out):
    return np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)

W1 = he_init(input_size, hidden1)
b1 = np.zeros((1, hidden1))
W2 = he_init(hidden1, hidden2)
b2 = np.zeros((1, hidden2))
W3 = he_init(hidden2, output_size)
b3 = np.zeros((1, output_size))

# Adam optimizer parameters
mW1 = np.zeros_like(W1); vW1 = np.zeros_like(W1)
mW2 = np.zeros_like(W2); vW2 = np.zeros_like(W2)
mW3 = np.zeros_like(W3); vW3 = np.zeros_like(W3)
mb1 = np.zeros_like(b1); vb1 = np.zeros_like(b1)
mb2 = np.zeros_like(b2); vb2 = np.zeros_like(b2)
mb3 = np.zeros_like(b3); vb3 = np.zeros_like(b3)

beta1 = 0.9
beta2 = 0.999
eps = 1e-8

# ------------------------------------------------
# 4. FORWARD PROPAGATION
# ------------------------------------------------
def forward(X, training=True):
    Z1 = X.dot(W1) + b1
    A1 = relu(Z1)

    if training:
        D1 = (np.random.rand(*A1.shape) > dropout_rate) / (1 - dropout_rate)
        A1 = A1 * D1
    else:
        D1 = 1

    Z2 = A1.dot(W2) + b2
    A2 = relu(Z2)

    if training:
        D2 = (np.random.rand(*A2.shape) > dropout_rate) / (1 - dropout_rate)
        A2 = A2 * D2
    else:
        D2 = 1

    Z3 = A2.dot(W3) + b3
    A3 = softmax(Z3)

    cache = (Z1, A1, D1, Z2, A2, D2, Z3, A3)
    return A3, cache

# ------------------------------------------------
# 5. LOSS FUNCTION (CROSS-ENTROPY + L2)
# ------------------------------------------------
def compute_loss(y, y_pred):
    m = y.shape[0]
    ce = -np.sum(y * np.log(y_pred + 1e-8)) / m
    l2 = l2_lambda * (np.sum(W1**2) + np.sum(W2**2) + np.sum(W3**2)) / 2
    return ce + l2

# ------------------------------------------------
# 6. TRAINING (BACKPROP + ADAM)
# ------------------------------------------------
loss_history = []
val_loss_history = []

for epoch in range(epochs):
    # Forward pass
    y_pred, cache = forward(X_train, training=True)
    Z1, A1, D1, Z2, A2, D2, Z3, A3 = cache
    m = y_train.shape[0]

    # Compute loss
    loss = compute_loss(y_train, y_pred)

    # ------------------------------
    # BACKPROPAGATION
    # ------------------------------
    dZ3 = (A3 - y_train) / m
    dW3 = A2.T.dot(dZ3) + l2_lambda * W3
    db3 = np.sum(dZ3, axis=0, keepdims=True)

    dA2 = dZ3.dot(W3.T) * D2
    dZ2 = dA2 * relu_derivative(Z2)
    dW2 = A1.T.dot(dZ2) + l2_lambda * W2
    db2 = np.sum(dZ2, axis=0, keepdims=True)

    dA1 = dZ2.dot(W2.T) * D1
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = X_train.T.dot(dZ1) + l2_lambda * W1
    db1 = np.sum(dZ1, axis=0, keepdims=True)

    # ------------------------------
    # ADAM OPTIMIZER UPDATE
    # ------------------------------
    for param, grad, m_t, v_t in [
        (W1, dW1, mW1, vW1),
        (W2, dW2, mW2, vW2),
        (W3, dW3, mW3, vW3),
        (b1, db1, mb1, vb1),
        (b2, db2, mb2, vb2),
        (b3, db3, mb3, vb3),
    ]:
        m_t[:] = beta1*m_t + (1-beta1)*grad
        v_t[:] = beta2*v_t + (1-beta2)*(grad**2)
        m_hat = m_t / (1 - beta1**(epoch+1))
        v_hat = v_t / (1 - beta2**(epoch+1))
        param -= lr * m_hat / (np.sqrt(v_hat) + eps)

    # ------------------------------
    # VALIDATION LOSS
    # ------------------------------
    val_pred, _ = forward(X_val, training=False)
    val_loss = compute_loss(y_val, val_pred)

    loss_history.append(loss)
    val_loss_history.append(val_loss)

    if epoch % 5 == 0:
        print(f"Epoch {epoch}: Train Loss={loss:.4f}, Val Loss={val_loss:.4f}")

# ------------------------------------------------
# 7. ACCURACY & PREDICTIONS
# ------------------------------------------------
def predict(X):
    probs, _ = forward(X, training=False)
    return np.argmax(probs, axis=1)

train_preds = predict(X_train)
val_preds   = predict(X_val)

train_acc = np.mean(train_preds == np.argmax(y_train, axis=1)) * 100
val_acc   = np.mean(val_preds == np.argmax(y_val, axis=1)) * 100

print("\nTrain Accuracy:", train_acc)
print("Validation Accuracy:", val_acc)

# ------------------------------------------------
# 8. CONFUSION MATRIX & CLASSIFICATION REPORT
# ------------------------------------------------
print("\nConfusion Matrix:")
print(confusion_matrix(np.argmax(y_val, axis=1), val_preds))

print("\nClassification Report:")
print(classification_report(np.argmax(y_val, axis=1), val_preds))

# ------------------------------------------------
# 9. PLOT LOSS CURVES
# ------------------------------------------------
plt.figure(figsize=(7,5))
plt.plot(loss_history, label="Train Loss")
plt.plot(val_loss_history, label="Validation Loss")
plt.title("Loss vs Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()


import numpy as np
import matplotlib.pyplot as plt

# Input values
x = np.linspace(-10, 10, 500)

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

# Compute outputs
y_sigmoid = sigmoid(x)
y_relu = relu(x)
y_tanh = tanh(x)

# Plotting
plt.figure(figsize=(12, 6))

plt.plot(x, y_sigmoid, label="Sigmoid", color="blue")
plt.plot(x, y_relu, label="ReLU", color="green")
plt.plot(x, y_tanh, label="Tanh", color="red")

plt.title("Activation Functions")
plt.xlabel("Input (x)")
plt.ylabel("Output")
plt.legend()
plt.grid(True)
plt.show()


# =========================================
# Experiment: Demonstration of Activation Functions
# Objective: Visualize Sigmoid, Tanh, and ReLU functions and their gradients
# =========================================

import numpy as np
import matplotlib.pyplot as plt

# 1Ô∏è‚É£ Input values
x = np.linspace(-10, 10, 500)

# 2Ô∏è‚É£ Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

# 3Ô∏è‚É£ Derivatives (Gradients)
def sigmoid_prime(x):
    s = sigmoid(x)
    return s * (1 - s)

def tanh_prime(x):
    return 1 - np.tanh(x)**2

def relu_prime(x):
    return (x > 0).astype(float)

# 4Ô∏è‚É£ Compute outputs
y_sigmoid = sigmoid(x)
y_tanh = tanh(x)
y_relu = relu(x)

dy_sigmoid = sigmoid_prime(x)
dy_tanh = tanh_prime(x)
dy_relu = relu_prime(x)

# 5Ô∏è‚É£ Plot Activation Functions
plt.figure(figsize=(12, 5))
plt.plot(x, y_sigmoid, label="Sigmoid", color="blue")
plt.plot(x, y_tanh, label="Tanh", color="red")
plt.plot(x, y_relu, label="ReLU", color="green")
plt.title("Activation Functions")
plt.xlabel("Input (x)")
plt.ylabel("Output")
plt.legend()
plt.grid(True)
plt.show()

# 6Ô∏è‚É£ Plot Gradients
plt.figure(figsize=(12, 5))
plt.plot(x, dy_sigmoid, label="Sigmoid'", color="blue")
plt.plot(x, dy_tanh, label="Tanh'", color="red")
plt.plot(x, dy_relu, label="ReLU'", color="green")
plt.title("Activation Function Gradients")
plt.xlabel("Input (x)")
plt.ylabel("Derivative")
plt.legend()
plt.grid(True)
plt.show()

# 7Ô∏è‚É£ Short Analysis
print("""
Analysis:
- Sigmoid and Tanh saturate for large positive/negative inputs ‚Üí gradients approach zero (vanishing gradient problem).
- Tanh is zero-centered, which can speed up training compared to Sigmoid.
- ReLU gradient is 1 for positive inputs and 0 for negative inputs ‚Üí avoids vanishing gradient for positive values.
- ReLU outputs zero for negative inputs ‚Üí sparsity, but can suffer from "dying ReLU" problem if neurons never activate.
""")


import random
import matplotlib.pyplot as plt

# -------------------------------
# Environment Class
# -------------------------------

class Environment:
    def __init__(self):
        # Rooms in the 2x2 grid
        self.rooms = {
            'A': random.choice(['Clean', 'Dirty']),
            'B': random.choice(['Clean', 'Dirty']),
            'C': random.choice(['Clean', 'Dirty']),
            'D': random.choice(['Clean', 'Dirty'])
        }
    
    def is_dirty(self, room):
        return self.rooms[room] == 'Dirty'
    
    def clean_room(self, room):
        self.rooms[room] = 'Clean'
    
    def regenerate_dirt(self, probability=0.2):
        """20% chance: clean rooms become dirty again"""
        for r in self.rooms:
            if self.rooms[r] == 'Clean' and random.random() < probability:
                self.rooms[r] = 'Dirty'

# -------------------------------
# Simple Reflex Agent
# -------------------------------

class VacuumAgent:
    def __init__(self, environment):
        self.env = environment
        self.current_room = random.choice(['A', 'B', 'C', 'D'])
        self.score = 0

    def sense(self):
        """Check if current room is dirty"""
        return self.env.is_dirty(self.current_room)

    def clean(self):
        self.env.clean_room(self.current_room)
        self.score += 10   # reward
        print(f"Cleaned {self.current_room} ‚Üí Score +10")

    def move(self):
        """Move to any random different room"""
        next_room = random.choice([r for r in self.env.rooms if r != self.current_room])
        print(f"Moved from {self.current_room} ‚Üí {next_room} (Cost -1)")
        self.current_room = next_room
        self.score -= 1   # movement cost

    def step(self):
        """One perception‚Äìaction cycle"""
        if self.sense():   # If dirty ‚Üí clean
            self.clean()
        else:              # If clean ‚Üí move
            self.move()

# -------------------------------
# Simulation Code
# -------------------------------

env = Environment()
agent = VacuumAgent(env)

performance_log = []
steps = 20  # number of simulation steps

print("\n--- STARTING ENVIRONMENT ---")
print("Initial Room Status:", env.rooms)
print("Starting in Room:", agent.current_room)
print("-----------------------------\n")

for t in range(steps):
    print(f"--- Step {t+1} ---")
    
    # Agent performs an action
    agent.step()

    # Environment regenerates dirt randomly
    env.regenerate_dirt()

    # Log performance
    performance_log.append(agent.score)

    print("Room Status:", env.rooms)
    print("Current Score:", agent.score)
    print("-----------------------------\n")

# -------------------------------
# Plot Performance Graph
# -------------------------------

plt.figure(figsize=(8,5))
plt.plot(range(1, steps+1), performance_log, marker='o')
plt.title("Agent Performance Over Time")
plt.xlabel("Simulation Step")
plt.ylabel("Performance Score")
plt.grid(True)
plt.show()

print("Final Score:", agent.score)


# ===============================================================
# LINEAR PROGRAMMING ‚Äî RESOURCE ALLOCATION OPTIMIZATION
# ===============================================================
# Maximize   Z = 70x1 + 80x2
# Subject to:
#           2x1 + 3x2 ‚â§ 450
#           3x1 + 2x2 ‚â§ 350
#           x1, x2 ‚â• 0
# ===============================================================

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import linprog
from mpl_toolkits.mplot3d import Axes3D

# ---------------------------------------------------------------
# 1. Define LP model
# ---------------------------------------------------------------

# Objective (convert max ‚Üí min by multiplying with -1)
c = [-70, -80]

# Inequality constraints A x ‚â§ b
A = [
    [2, 3],
    [3, 2]
]
b = [450, 350]

# Bounds (x1, x2 ‚â• 0)
x_bounds = (0, None)
y_bounds = (0, None)

# ---------------------------------------------------------------
# 2. Solve using HiGHS
# ---------------------------------------------------------------
res = linprog(c, A_ub=A, b_ub=b, bounds=[x_bounds, y_bounds], method="highs")

x1_opt, x2_opt = res.x
max_profit = -(res.fun)

print("Optimal Solution:")
print(f"x1 = {x1_opt:.2f}")
print(f"x2 = {x2_opt:.2f}")
print(f"Maximum Profit = {max_profit:.2f}")

# ---------------------------------------------------------------
# 3. 2D Feasible Region Plot
# ---------------------------------------------------------------
x = np.linspace(0, 250, 400)

c1 = (450 - 2*x) / 3
c2 = (350 - 3*x) / 2

plt.figure(figsize=(7,6))
plt.plot(x, c1, label=r"$2x_1 + 3x_2 = 450$")
plt.plot(x, c2, label=r"$3x_1 + 2x_2 = 350$")

plt.fill_between(x, np.minimum(c1, c2), where=(np.minimum(c1, c2)>=0), color='lightblue', alpha=0.4)

plt.scatter(x1_opt, x2_opt, color='red', s=80, label="Optimal Point")
plt.text(x1_opt+2, x2_opt+2, f"({x1_opt:.1f}, {x2_opt:.1f})", color='red')

plt.xlim(0, 200)
plt.ylim(0, 200)
plt.xlabel("x‚ÇÅ (Units of Product 1)")
plt.ylabel("x‚ÇÇ (Units of Product 2)")
plt.title("Feasible Region for Resource Allocation Problem")
plt.legend()
plt.grid(True)
plt.show()

# ---------------------------------------------------------------
# 4. 3D Profit Surface Visualization
# ---------------------------------------------------------------
fig = plt.figure(figsize=(10,7))
ax = fig.add_subplot(111, projection='3d')

X = np.linspace(0, 200, 80)
Y = np.linspace(0, 200, 80)
XX, YY = np.meshgrid(X, Y)

ZZ = 70*XX + 80*YY  # profit function

ax.plot_surface(XX, YY, ZZ, cmap='viridis', alpha=0.75)

# Constraint planes (boundary)
c1_line = 450 - 2*X
c2_line = 350 - 3*X

# Add optimal point
ax.scatter(x1_opt, x2_opt, max_profit, color='red', s=60)
ax.text(x1_opt, x2_opt, max_profit, " Optimal", color='red')

ax.set_xlabel("x‚ÇÅ")
ax.set_ylabel("x‚ÇÇ")
ax.set_zlabel("Profit Z")
ax.set_title("3D Profit Surface and Optimal Solution")

plt.show()
